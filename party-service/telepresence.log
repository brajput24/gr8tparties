   0.0 TEL | Telepresence 0.88 launched at Tue May 29 07:46:50 2018
   0.0 TEL |   /usr/local/bin/telepresence --namespace cesar --mount=/tmp/known --swap-deployment party-service --docker-run --rm -v=/tmp/known/var/run/secrets:/var/run/secrets -v/Users/cesartron-lozai/repos/java/gr8tparties/party-service:/build -v /Users/cesartron-lozai/.m2/repository:/m2 -v /Users/cesartron-lozai/jrebel-nightly:/jrebel -v /Users/cesartron-lozai/jrebel-nightly/jrebel.jar:/jrebel.jar -v /Users/cesartron-lozai/.jrebel:/root/.jrebel -p 8080:8080 -p 5005:5005 -e MAVEN_OPTS=-agentpath:/jrebel/lib/libjrebel64.so 738398925563.dkr.ecr.eu-west-2.amazonaws.com/maven-build:jdk10 mvn -Dmaven.repo.local=/m2 -f /build spring-boot:run
   0.2 TEL | Context: k8s-dev.engineering.convendia.com, namespace: cesar, kubectl_command: kubectl
   0.2 TEL | [1] Launching: kubectl version --short
   0.2 TEL | [2] Launching: oc version
   0.2 TEL | [2] [Errno 2] No such file or directory: 'oc': 'oc'
   0.2 TEL | [3] Launching: uname -a
   0.2 TEL | Python 3.6.5 (default, Mar 30 2018, 06:41:53)
   0.2 TEL | [GCC 4.2.1 Compatible Apple LLVM 9.0.0 (clang-900.0.39.2)]
   0.3 TEL | [4] Capturing: kubectl --context k8s-dev.engineering.convendia.com --namespace cesar get pods telepresence-connectivity-check --ignore-not-found
   0.3   3 | Darwin Cesars-MBP.home 17.5.0 Darwin Kernel Version 17.5.0: Mon Mar  5 22:24:32 PST 2018; root:xnu-4570.51.1~1/RELEASE_X86_64 x86_64
   0.3 TEL | [3] exit 0
   0.4   1 | Client Version: v1.10.1
   0.4   1 | Server Version: v1.10.2
   0.4 TEL | [1] exit 0
   0.4 TEL | [5] Capturing: ssh -V
   0.4 TEL | [6] Capturing: which torsocks
   0.4 TEL | [7] Capturing: which sshfs
   0.4 TEL | BEGIN SPAN main.py:51(main)
   1.2 TEL | Scout info: {'latest_version': '0.88', 'application': 'telepresence', 'notices': []}
   1.2 TEL | BEGIN SPAN proxy.py:150(start_proxy)
   1.2 TEL | BEGIN SPAN deployment.py:92(supplant_deployment)
   1.2 TEL | BEGIN SPAN remote.py:82(get_deployment_json)
   1.2 TEL | [8] Capturing: kubectl --context k8s-dev.engineering.convendia.com --namespace cesar get deployment -o json --export party-service
   1.3 TEL | END SPAN remote.py:82(get_deployment_json)    0.1s
   1.3 TEL | [9] Running: kubectl --context k8s-dev.engineering.convendia.com --namespace cesar delete deployment party-service-252b0655-1877-44dd-934e-b16e95ac5266 --ignore-not-found
   1.4 TEL | [10] Running: kubectl --context k8s-dev.engineering.convendia.com --namespace cesar apply -f -
   1.6  10 | deployment.extensions "party-service-252b0655-1877-44dd-934e-b16e95ac5266" created
   1.6 TEL | [11] Running: kubectl --context k8s-dev.engineering.convendia.com --namespace cesar scale deployment party-service --replicas=0
   1.8  11 | deployment.extensions "party-service" scaled
   1.8 TEL | END SPAN deployment.py:92(supplant_deployment)    0.6s
   1.8 TEL | BEGIN SPAN remote.py:164(get_remote_info)
   1.8 TEL | BEGIN SPAN remote.py:82(get_deployment_json)
   1.8 TEL | [12] Capturing: kubectl --context k8s-dev.engineering.convendia.com --namespace cesar get deployment -o json --export --selector=telepresence=252b0655-1877-44dd-934e-b16e95ac5266
   1.9 TEL | END SPAN remote.py:82(get_deployment_json)    0.1s
   1.9 TEL | Searching for Telepresence pod:
   1.9 TEL |   with name party-service-252b0655-1877-44dd-934e-b16e95ac5266-*
   1.9 TEL |   with labels {'app': 'party-service', 'telepresence': '252b0655-1877-44dd-934e-b16e95ac5266'}
   1.9 TEL |   with namespace cesar
   1.9 TEL | [13] Capturing: kubectl --context k8s-dev.engineering.convendia.com --namespace cesar get pod -o json --export --selector=telepresence=252b0655-1877-44dd-934e-b16e95ac5266
   2.0 TEL | Checking party-service-252b0655-1877-44dd-934e-b16e95ac5266-667b774xtm8j
   2.0 TEL | Looks like we've found our pod!
   2.0 TEL | BEGIN SPAN remote.py:122(wait_for_pod)
   2.0 TEL | [14] Capturing: kubectl --context k8s-dev.engineering.convendia.com --namespace cesar get pod party-service-252b0655-1877-44dd-934e-b16e95ac5266-667b774xtm8j -o json
   2.4 TEL | [15] Capturing: kubectl --context k8s-dev.engineering.convendia.com --namespace cesar get pod party-service-252b0655-1877-44dd-934e-b16e95ac5266-667b774xtm8j -o json
   2.8 TEL | [16] Capturing: kubectl --context k8s-dev.engineering.convendia.com --namespace cesar get pod party-service-252b0655-1877-44dd-934e-b16e95ac5266-667b774xtm8j -o json
   3.1 TEL | [17] Capturing: kubectl --context k8s-dev.engineering.convendia.com --namespace cesar get pod party-service-252b0655-1877-44dd-934e-b16e95ac5266-667b774xtm8j -o json
   3.6 TEL | [18] Capturing: kubectl --context k8s-dev.engineering.convendia.com --namespace cesar get pod party-service-252b0655-1877-44dd-934e-b16e95ac5266-667b774xtm8j -o json
   3.8 TEL | END SPAN remote.py:122(wait_for_pod)    1.8s
   3.8 TEL | END SPAN remote.py:164(get_remote_info)    2.1s
   3.8 TEL | END SPAN proxy.py:150(start_proxy)    2.7s
   3.8 TEL | BEGIN SPAN proxy.py:42(connect)
   3.8 TEL | [19] Launching: kubectl --context k8s-dev.engineering.convendia.com --namespace cesar logs -f party-service-252b0655-1877-44dd-934e-b16e95ac5266-667b774xtm8j --container party-service
   3.8 TEL | [20] Launching: kubectl --context k8s-dev.engineering.convendia.com --namespace cesar port-forward party-service-252b0655-1877-44dd-934e-b16e95ac5266-667b774xtm8j 49184:8022
   3.9 TEL | [21] Running: sudo ifconfig lo0 alias 198.18.0.254
   3.9 TEL | [22] Launching: socat TCP4-LISTEN:49184,bind=198.18.0.254,reuseaddr,fork TCP4:127.0.0.1:49184
   3.9 TEL | [23] Running: ssh -F /dev/null -q -oStrictHostKeyChecking=no -oUserKnownHostsFile=/dev/null -p 49184 telepresence@localhost /bin/true
   3.9 TEL | [23] exit 255 in 0.02 secs.
   4.0  19 | Listening...
   4.0  19 | 2018-05-29T06:46:54+0000 [-] Loading ./forwarder.py...
   4.0  19 | 2018-05-29T06:46:54+0000 [-] /etc/resolv.conf changed, reparsing
   4.0  19 | 2018-05-29T06:46:54+0000 [-] Resolver added ('100.64.0.10', 53) to server list
   4.0  19 | 2018-05-29T06:46:54+0000 [-] SOCKSv5Factory starting on 9050
   4.0  19 | 2018-05-29T06:46:54+0000 [socks.SOCKSv5Factory#info] Starting factory <socks.SOCKSv5Factory object at 0x7ff71a008c18>
   4.0  19 | 2018-05-29T06:46:54+0000 [-] DNSDatagramProtocol starting on 9053
   4.0  19 | 2018-05-29T06:46:54+0000 [-] Starting protocol <twisted.names.dns.DNSDatagramProtocol object at 0x7ff71a008ef0>
   4.0  19 | 2018-05-29T06:46:54+0000 [-] Loaded.
   4.0  19 | 2018-05-29T06:46:54+0000 [twisted.scripts._twistd_unix.UnixAppLogger#info] twistd 18.4.0 (/usr/bin/python3.6 3.6.1) starting up.
   4.0  19 | 2018-05-29T06:46:54+0000 [twisted.scripts._twistd_unix.UnixAppLogger#info] reactor class: twisted.internet.epollreactor.EPollReactor.
   4.1  20 | Forwarding from 127.0.0.1:49184 -> 8022
   4.1  20 | Forwarding from [::1]:49184 -> 8022
   4.2 TEL | [24] Running: ssh -F /dev/null -q -oStrictHostKeyChecking=no -oUserKnownHostsFile=/dev/null -p 49184 telepresence@localhost /bin/true
   4.2  20 | Handling connection for 49184
   4.3 TEL | [25] Launching: ssh -N -oServerAliveInterval=1 -oServerAliveCountMax=10 -F /dev/null -q -oStrictHostKeyChecking=no -oUserKnownHostsFile=/dev/null -p 49184 telepresence@localhost -L127.0.0.1:49199:127.0.0.1:9050 -R9055:127.0.0.1:49200
   4.3 TEL | END SPAN proxy.py:42(connect)    0.5s
   4.3 TEL | BEGIN SPAN remote_env.py:46(get_env_variables)
   4.3 TEL | [26] Capturing: kubectl --context k8s-dev.engineering.convendia.com --namespace cesar exec party-service-252b0655-1877-44dd-934e-b16e95ac5266-667b774xtm8j --container party-service -- python3 -c 'import json, os; print(json.dumps(dict(os.environ)))'
   4.4  20 | Handling connection for 49184
   4.8 TEL | END SPAN remote_env.py:46(get_env_variables)    0.4s
   4.8 TEL | BEGIN SPAN mount.py:36(mount_remote_volumes)
   4.8 TEL | [27] Capturing: sudo sshfs -p 49184 -F /dev/null -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -o allow_other telepresence@localhost:/ /tmp/known
   4.8  20 | Handling connection for 49184
   5.0 TEL | END SPAN mount.py:36(mount_remote_volumes)    0.2s
   5.0 TEL | BEGIN SPAN vpn.py:84(get_proxy_cidrs)
   5.0 TEL | END SPAN vpn.py:84(get_proxy_cidrs)    0.0s
   5.0 TEL | BEGIN SPAN container.py:113(run_docker_command)
   5.0 TEL | [28] Launching: docker run -p=8080:8080 -p=5005:5005 --rm --privileged --name=telepresence-1527576415-3769522-93510 datawire/telepresence-local:0.88 proxy '{"port": 49184, "cidrs": ["100.96.5.0/24", "100.96.2.0/24", "100.64.0.0/13"], "expose_ports": [[8080, 8080]], "ip": "198.18.0.254"}'
   5.0 TEL | [29] Running: docker run --network=container:telepresence-1527576415-3769522-93510 --rm datawire/telepresence-local:0.88 wait
   5.6  28 | [INFO  tini (1)] Spawned child process 'python3' with pid '7'
   5.7  28 |    0.0 TEL | Telepresence 0.88 launched at Tue May 29 06:46:54 2018
   5.7  28 |    0.0 TEL |   /usr/bin/entrypoint.py proxy '{"port": 49184, "cidrs": ["100.96.5.0/24", "100.96.2.0/24", "100.64.0.0/13"], "expose_ports": [[8080, 8080]], "ip": "198.18.0.254"}'
   5.7  28 |    0.0 TEL | [1] Launching: kubectl version --short
   5.7  28 |    0.0 TEL | [1] [Errno 2] No such file or directory: 'kubectl'
   5.7  28 |    0.0 TEL | [2] Launching: oc version
   5.7  28 |    0.0 TEL | [2] [Errno 2] No such file or directory: 'oc'
   5.7  28 |    0.0 TEL | [3] Launching: uname -a
   5.7  28 |    0.0   3 | Linux c1fc04d601f7 4.9.87-linuxkit-aufs #1 SMP Wed Mar 14 15:12:16 UTC 2018 x86_64 Linux
   5.7  28 |    0.0 TEL | Python 3.6.1 (default, Oct  2 2017, 20:46:59)
   5.7  28 |    0.0 TEL | [GCC 6.3.0]
   5.7  28 |    0.0 TEL | [3] exit 0
   5.7  28 |    0.0 TEL | [4] Launching: ssh -N -oServerAliveInterval=1 -oServerAliveCountMax=10 -F /dev/null -q -oStrictHostKeyChecking=no -oUserKnownHostsFile=/dev/null -p 49184 telepresence@198.18.0.254 -R '*:8080:127.0.0.1:8080'
   5.7  28 |    0.0 TEL | Everything launched. Waiting to exit...
   5.7  20 | Handling connection for 49184
   5.7  28 |    0.0 TEL | BEGIN SPAN cleanup.py:98(wait_for_exit)
   5.8  29 | [INFO  tini (1)] Spawned child process 'python3' with pid '7'
   5.9  28 | Starting sshuttle proxy.
   6.1  28 | firewall manager: Starting firewall with Python version 3.6.1
   6.1  28 | firewall manager: ready method name nat.
   6.1  28 | IPv6 enabled: False
   6.1  28 | UDP enabled: False
   6.1  28 | DNS enabled: True
   6.1  28 | TCP redirector listening on ('127.0.0.1', 12300).
   6.1  28 | DNS listening on ('127.0.0.1', 12300).
   6.1  28 | Starting client with Python version 3.6.1
   6.1  28 | c : connecting to server...
   6.1  20 | Handling connection for 49184
   6.1  28 | Warning: Permanently added '[198.18.0.254]:49184' (ECDSA) to the list of known hosts.
   6.3  28 | Starting server with Python version 3.6.1
   6.3  28 |  s: latency control setting = True
   6.3  28 |  s: available routes:
   6.3  28 |  s:   2/100.96.5.0/24
   6.3  28 | c : Connected.
   6.3  28 | firewall manager: setting up.
   6.3  28 | >> iptables -t nat -N sshuttle-12300
   6.3  28 | >> iptables -t nat -F sshuttle-12300
   6.3  28 | >> iptables -t nat -I OUTPUT 1 -j sshuttle-12300
   6.3  28 | >> iptables -t nat -I PREROUTING 1 -j sshuttle-12300
   6.3  28 | >> iptables -t nat -A sshuttle-12300 -j RETURN --dest 127.0.0.1/32 -p tcp
   6.3  28 | >> iptables -t nat -A sshuttle-12300 -j REDIRECT --dest 100.96.5.0/24 -p tcp --to-ports 12300 -m ttl ! --ttl 42
   6.3  28 | >> iptables -t nat -A sshuttle-12300 -j REDIRECT --dest 100.96.2.0/24 -p tcp --to-ports 12300 -m ttl ! --ttl 42
   6.3  28 | >> iptables -t nat -A sshuttle-12300 -j REDIRECT --dest 100.64.0.0/13 -p tcp --to-ports 12300 -m ttl ! --ttl 42
   6.3  28 | >> iptables -t nat -A sshuttle-12300 -j REDIRECT --dest 192.168.65.1/32 -p udp --dport 53 --to-ports 12300 -m ttl ! --ttl 42
   6.3  28 | >> iptables -t nat -A sshuttle-12300 -j REDIRECT --dest 224.0.0.252/32 -p udp --dport 5355 --to-ports 12300 -m ttl ! --ttl 42
   6.3  28 | conntrack v1.4.4 (conntrack-tools): 0 flow entries have been deleted.
   6.9  29 | [INFO  tini (1)] Main child exited normally (with status '100')
   7.1 TEL | [29] exit 100 in 2.10 secs.
   7.1 TEL | [30] Capturing: docker run --help
   7.2 TEL | END SPAN container.py:113(run_docker_command)    2.2s
   7.2 TEL | Everything launched. Waiting to exit...
   7.2 TEL | BEGIN SPAN cleanup.py:98(wait_for_exit)
  10.8  28 | c : DNS request from ('172.17.0.2', 43259) to None: 43 bytes
  10.8  19 | 2018-05-29T06:47:01+0000 [stdout#info] A query: b'update.zeroturnaround.com'
  10.8  19 | 2018-05-29T06:47:01+0000 [stdout#info] Result for b'update.zeroturnaround.com' is ['52.6.43.173', '34.197.61.177']
  30.1  28 | c : DNS request from ('172.17.0.2', 34719) to None: 32 bytes
  30.1  19 | 2018-05-29T06:47:20+0000 [stdout#info] A query: b'api.segment.io'
  30.1  19 | 2018-05-29T06:47:20+0000 [stdout#info] Result for b'api.segment.io' is ['54.148.40.191', '54.148.108.154', '54.148.239.244', '54.186.59.112', '54.186.200.253', '54.187.42.35', '52.40.189.95', '52.88.146.11']
  33.8 TEL | (proxy checking local liveness)
  33.8  19 | 2018-05-29T06:47:24+0000 [Poll#info] Checkpoint
  43.1  28 | c : DNS request from ('172.17.0.2', 57445) to None: 40 bytes
  43.1  19 | 2018-05-29T06:47:33+0000 [stdout#info] A query: b'kubernetes.default.svc'
  43.1  19 | 2018-05-29T06:47:33+0000 [stdout#info] Result for b'kubernetes.default.svc' is ['100.64.0.1']
  43.2  28 | c : Accept TCP: 172.17.0.2:36920 -> 100.64.0.1:443.
  46.1  28 | c : Accept TCP: 172.17.0.2:36922 -> 100.64.0.1:443.
  46.5  28 | c : Accept TCP: 172.17.0.2:36924 -> 100.64.0.1:443.
  55.8  28 | c : DNS request from ('172.17.0.2', 57752) to None: 39 bytes
  55.8  19 | 2018-05-29T06:47:46+0000 [stdout#info] A query: b'mongodb.convendia.com'
  55.9  19 | 2018-05-29T06:47:46+0000 [stdout#info] getaddrinfo error: [Errno -2] Name does not resolve
  55.9  28 | c : DNS request from ('172.17.0.2', 40084) to None: 25 bytes
  55.9  19 | 2018-05-29T06:47:46+0000 [stdout#info] A query: b'mongodb'
  55.9  19 | 2018-05-29T06:47:46+0000 [stdout#info] Result for b'mongodb' is ['100.65.144.143']
  55.9  28 | c : Accept TCP: 172.17.0.2:44654 -> 100.65.144.143:27017.
  63.4  28 | c : Accept TCP: 172.17.0.2:36930 -> 100.64.0.1:443.
  63.8 TEL | (proxy checking local liveness)
  63.8  19 | 2018-05-29T06:47:54+0000 [Poll#info] Checkpoint
  81.9  28 | c : Accept TCP: 172.17.0.2:38028 -> 100.96.5.75:8080.
  82.7  28 | c : Accept TCP: 172.17.0.2:44664 -> 100.65.144.143:27017.
  93.8 TEL | (proxy checking local liveness)
  93.8  19 | 2018-05-29T06:48:24+0000 [Poll#info] Checkpoint
 123.8 TEL | (proxy checking local liveness)
 123.8  19 | 2018-05-29T06:48:54+0000 [Poll#info] Checkpoint
 146.0  28 |  s: SW'unknown':Mux#11: deleting (13 remain)
 146.0  28 |  s: SW#12:100.96.5.75:8080: deleting (12 remain)
 153.8 TEL | (proxy checking local liveness)
 153.9  19 | 2018-05-29T06:49:24+0000 [Poll#info] Checkpoint
 183.8 TEL | (proxy checking local liveness)
 183.8  19 | 2018-05-29T06:49:54+0000 [Poll#info] Checkpoint
 213.8 TEL | (proxy checking local liveness)
 213.8  19 | 2018-05-29T06:50:24+0000 [Poll#info] Checkpoint
 243.8 TEL | (proxy checking local liveness)
 243.8  19 | 2018-05-29T06:50:54+0000 [Poll#info] Checkpoint
 273.8 TEL | (proxy checking local liveness)
 273.8  19 | 2018-05-29T06:51:24+0000 [Poll#info] Checkpoint
 303.9 TEL | (proxy checking local liveness)
 303.9  19 | 2018-05-29T06:51:54+0000 [Poll#info] Checkpoint
 333.9 TEL | (proxy checking local liveness)
 333.9  19 | 2018-05-29T06:52:24+0000 [Poll#info] Checkpoint
 363.2  28 |  s: SW'unknown':Mux#4: deleting (11 remain)
 363.2  28 |  s: SW#7:100.64.0.1:443: deleting (10 remain)
 363.2  28 |  s: SW'unknown':Mux#6: deleting (9 remain)
 363.2  28 |  s: SW#9:100.64.0.1:443: deleting (8 remain)
 363.8 TEL | (proxy checking local liveness)
 363.8  19 | 2018-05-29T06:52:54+0000 [Poll#info] Checkpoint
 376.0  28 | c : SW#11:172.17.0.2:36924: deleting (13 remain)
 376.0  28 | c : SW#11:172.17.0.2:36924: error was: nowrite: [Errno 107] Socket not connected
 376.0  28 | c : SW'unknown':Mux#6: deleting (12 remain)
 393.8 TEL | (proxy checking local liveness)
 393.8  19 | 2018-05-29T06:53:24+0000 [Poll#info] Checkpoint
 423.8 TEL | (proxy checking local liveness)
 423.8  19 | 2018-05-29T06:53:54+0000 [Poll#info] Checkpoint
 454.0 TEL | (proxy checking local liveness)
 454.0  19 | 2018-05-29T06:54:24+0000 [Poll#info] Checkpoint
 466.0  28 |  s: SW'unknown':Mux#10: deleting (7 remain)
 466.0  28 |  s: SW#10:100.64.0.1:443: deleting (6 remain)
 483.8 TEL | (proxy checking local liveness)
 483.8  19 | 2018-05-29T06:54:54+0000 [Poll#info] Checkpoint
 513.8 TEL | (proxy checking local liveness)
 513.8  19 | 2018-05-29T06:55:24+0000 [Poll#info] Checkpoint
 534.3 TEL | Shutting down containers...
 534.3 TEL | Killing local container...
 534.3 TEL | [31] Running: docker stop --time=1 telepresence-1527576417-483716-93510
 534.3 TEL | [25] exit 0
 534.3 TEL | [22] exit 130
 534.3  28 |  529.3   4 | Connection to 198.18.0.254 closed by remote host.
 534.3  28 | Connection to 198.18.0.254 closed by remote host.
 534.3 TEL | [19] exit -2
 534.3  28 |  529.3 TEL | [4] exit 255
 534.3 TEL | [20] exit 0
 534.3  28 | >> iptables -t nat -D OUTPUT -j sshuttle-12300
 534.3  28 | >> iptables -t nat -D PREROUTING -j sshuttle-12300
 534.3  28 | >> iptables -t nat -F sshuttle-12300
 534.3  28 | >> iptables -t nat -X sshuttle-12300
 534.3  28 | firewall manager: Error trying to undo /etc/hosts changes.
 534.3  28 | firewall manager: ---> Traceback (most recent call last):
 534.3  28 | firewall manager: --->   File "/usr/lib/python3.6/site-packages/sshuttle/firewall.py", line 274, in main
 534.3  28 | firewall manager: --->     restore_etc_hosts(port_v6 or port_v4)
 534.3  28 | firewall manager: --->   File "/usr/lib/python3.6/site-packages/sshuttle/firewall.py", line 50, in restore_etc_hosts
 534.3  28 | firewall manager: --->     rewrite_etc_hosts({}, port)
 534.3  28 | firewall manager: --->   File "/usr/lib/python3.6/site-packages/sshuttle/firewall.py", line 29, in rewrite_etc_hosts
 534.3  28 | firewall manager: --->     os.link(HOSTSFILE, BAKFILE)
 534.3  28 | firewall manager: ---> OSError: [Errno 18] Cross-device link: '/etc/hosts' -> '/etc/hosts.sbak'
 534.3  28 | Traceback (most recent call last):
 534.3  28 |   File "/usr/bin/entrypoint.py", line 123, in <module>
 534.3  28 |     main()
 534.3  28 |   File "/usr/bin/entrypoint.py", line 66, in main
 534.3  28 |     proxy(loads(sys.argv[2]))
 534.3  28 |   File "/usr/bin/entrypoint.py", line 106, in proxy
 534.3  28 |     wait_for_exit(runner, main_process, subps)
 534.3  28 |   File "/usr/lib/python3.6/site-packages/telepresence/cleanup.py", line 100, in wait_for_exit
 534.3  28 |     sleep(0.1)
 534.3  28 | KeyboardInterrupt
 534.4  28 | [INFO  tini (1)] Main child exited normally (with status '1')
 535.1 TEL | [28] exit 1
 535.6  31 | telepresence-1527576417-483716-93510
 535.6 TEL | [31] ran in 1.33 secs.
 535.6 TEL | [32] Capturing: sudo umount -f /tmp/known
 539.5  32 | umount: /tmp/known: not currently mounted
 539.5 TEL | [32] exit 1 in 3.84 secs.
 539.5 TEL | [33] Running: sudo ifconfig lo0 -alias 198.18.0.254
 540.0 TEL | [34] Running: docker stop --time=1 telepresence-1527576415-3769522-93510
 540.1  34 | Error response from daemon: No such container: telepresence-1527576415-3769522-93510
 540.1 TEL | [34] exit 1 in 0.07 secs.
 540.1 TEL | [35] Running: kubectl --context k8s-dev.engineering.convendia.com --namespace cesar scale deployment party-service --replicas=1
 540.2  35 | deployment.extensions "party-service" scaled
 540.2 TEL | [36] Running: kubectl --context k8s-dev.engineering.convendia.com --namespace cesar delete deployment party-service-252b0655-1877-44dd-934e-b16e95ac5266
 541.5  36 | deployment.extensions "party-service-252b0655-1877-44dd-934e-b16e95ac5266" deleted
 541.5 TEL | [36] ran in 1.27 secs.
 541.5 TEL | END SPAN main.py:51(main)  541.0s
